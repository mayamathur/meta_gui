{
    "contents" : "\nlibrary(shiny)\nlibrary(ggplot2)\n\n\n# LATER SOURCE EVALUE INSTEAD\n#library(ConfoundedMeta)\n#source( \"~/Dropbox/Personal computer/Independent studies/EValue package/EValue/R/EValue.R\" )\n\n\n#' An example dataset\n#'\n#' An example dataset from Hsu and Small (Biometrics, 2013). \n#'\n#' @docType data\n#' @keywords datasets\n\"lead\"\n\n\n\n# #' Compute E-value from a linear regression z-score\n# #' \n# #' Given an estimated coefficient with its standard error from a linear\n# #' regression model, or alternatively the associated two-sided p-value, \n# #' computes an approximate E-value through conversion to the Fisher's scale\n# #' and then to the standardized mean difference (Cohen's d) scale.\n# #' Assumes that the independent and dependent variables are bivariate normal. \n# #' @param beta The estimated regression coefficient.\n# #' Can be left empty if providing both a p-value and a sample size.\n# #' @param se The standard error of the regression coefficient.\n# #' Can be left empty if providing both a p-value and a sample size. \n# #' @param pval The two-sided p-value of the estimated regression coefficient. \n# #' Can be left empty if providing both an estimated coefficient and \n# #' the associated standard error. \n# #' @param n The total sample size in the regression model. \n# #' Can be left empty if providing both an estimated coefficient and \n# #' the associated standard error. \n# #' @param true The true standardized mean difference to which to shift the observed point estimate. Typically set to 1 to consider a null true effect. \n# #' @export\n# #' @examples\n# #' # using estimated coefficient and SE\n# #' evaluesrG( beta = 1.4, se = .5, n = 100 )\n# #' \n# #' # using p-value\n# #' evaluesrG( pval = 0.03, n = 100 )\n# \n# evaluesrG = function( beta=NA, se = NA, pval = NA, n = NA, true = 0 ) {\n#   \n#   if ( is.na(n) ) stop(\"Must provide n.\")\n#   \n#   # if user didn't pass \n#   if ( ( is.na(beta) | is.na(se) ) & is.na(pval) ) {\n#     stop(\"Must provide either: 1) beta and se; or 2) pval.\")\n#   }\n#   \n#   # get z-score and sample size from provided information\n#   if ( ! is.na(beta) ) z = beta / se\n#   else if ( ! is.na(pval) ) z = abs( qnorm( pval / 2 ) )\n#   \n#   # convert to mean difference\n#   SMD = sinh( 2 * z / sqrt( n - 3 ) )\n#   SE.SMD = 2 * sqrt( ( cosh( z / sqrt(n-3) ) )^4 / (n-2) )\n#   \n#   # to match the use of absolute value in z-score above\n#   true = abs(true)\n# \n#   return( evalues.MD( est = SMD, se = SE.SMD, true = true ) )\n# }\n\n\n\n#' Compute E-value for a difference of means and its confidence interval limits\n#' \n#' Returns a data frame containing point estimates, the lower confidence limit, and the upper confidence limit\n#' on the risk ratio scale (through an approximate conversion) as well as E-values for the point estimate and the confidence interval\n#' limit closer to the null.  \n#' @param est The point estimate as a standardized difference (i.e., Cohen's d)\n#' @param se The standard error of the point estimate\n#' @param true The true standardized mean difference to which to shift the observed point estimate. Typically set to 0 to consider a null true effect. \n#' @export\n#' @examples\n#' # compute E-value if Cohen's d = 0.5 with SE = 0.25\n#' evalues.MD( .5, .25 )\n\nevalues.MD = function( est, se = NA, true = 0 ) {\n    \n    values = c()\n    values[1] = exp( 0.91 * est )\n    values[2] = exp( 0.91 * est - 1.78 * se ) \n    values[3] = exp( 0.91 * est + 1.78 * se )\n    \n    # convert true value to which to shift observed point estimate\n    #  to RR scale\n    truerR = exp( 0.91 * true )\n    \n    return( evalues.RR( values[1], values[2], values[3], true = truerR ) )\n}\n\n\n\n#' Compute E-value for a hazard ratio and its confidence interval limits\n#' \n#' Returns a data frame containing point estimates, the lower confidence limit, and the upper confidence limit\n#' on the risk ratio scale (through an approximate conversion if needed when outcome is common) as well as E-values for the point estimate and the confidence interval\n#' limit closer to the null.  \n#' @param est The point estimate\n#' @param lo The lower limit of the confidence interval\n#' @param hi The upper limit of the confidence interval\n#' @param rare 1 if outcome is rare (<15 percent at end of follow-up); 0 if outcome is not rare (>15 percent at end of follow-up)\n#' @param true The true HR to which to shift the observed point estimate. Typically set to 1 to consider a null true effect. \n#' @export\n#' @examples\n#' # compute E-value for HR = 0.56 with CI: [0.46, 0.69]\n#' # for a common outcome\n#' evalues.HR( 0.56, 0.46, 0.69, rare = FALSE )\n\n\nevalues.HR = function( est, lo = NULL, hi = NULL, rare = NULL, true = 1 ) {\n    \n    # organize user's values\n    values = c(est, lo, hi)\n    \n    # sanity checks\n    if ( est < 0 ) stop(\"HR cannot be negative\")\n    if ( is.null(rare) ) stop(\"Must specify whether outcome is rare\")\n    \n    # if needed, convert to approximate RR\n    # if not rare, no conversion needed\n    if (rare){ \n        #warning(\"Results assume a rare outcome (<15% at end of follow-up)\")\n        truerR = true\n    }\n    \n    else if ( ! rare ) {\n        #warning(\"Results assume a common outcome (>15% at end of follow-up)\")\n        values = ( 1 - 0.5^sqrt( values ) ) / ( 1 - 0.5^sqrt( 1 / values ) )\n        \n        # convert true value to which to shift observed point estimate\n        #  to RR scale\n        truerR = ( 1 - 0.5^sqrt( true ) ) / ( 1 - 0.5^sqrt( 1 / true ) )\n    }\n    \n    return( evalues.RR( values[1], values[2], values[3], true = truerR ) )\n}\n\n\n\n#' Compute E-value for an odds ratio and its confidence interval limits\n#' \n#' Returns a data frame containing point estimates, the lower confidence limit, and the upper confidence limit\n#' on the risk ratio scale (through an approximate conversion if needed when outcome is common) as well as E-values for the point estimate and the confidence interval\n#' limit closer to the null.  \n#' @param est The point estimate\n#' @param lo The lower limit of the confidence interval\n#' @param hi The upper limit of the confidence interval \n#' @param rare 1 if outcome is rare (<15 percent at end of follow-up); 0 if outcome is not rare (>15 percent at end of follow-up)\n#' @param true The true OR to which to shift the observed point estimate. Typically set to 1 to consider a null true effect. \n#' @export\n#' @examples\n#' # compute E-values for OR = 0.86 with CI: [0.75, 0.99]\n#' # for a common outcome\n#' evalues.OR( 0.86, 0.75, 0.99, rare = FALSE )\n#' \n#' ## Example 2\n#' ## Hsu and Small (2013 Biometrics) Data\n#' ## sensitivity analysis after log-linear or logistic regression\n#' \n#' head(lead)\n#' \n#' ## log linear model -- obtain the conditional risk ratio\n#' lead.loglinear = glm(lead ~ ., family = binomial(link = \"log\"), \n#'                          data = lead)\n#' est = summary( lead.loglinear )$coef[2, c(1, 2)]\n#' \n#' RR       = exp(est[1])\n#' lowerRR  = exp(est[1] - 1.96*est[2])\n#' upperRR  = exp(est[1] + 1.96*est[2]) \n#' evalues.RR(RR, lowerRR, upperRR)\n#' \n#' ## logistic regression -- obtain the conditional odds ratio\n#' lead.logistic = glm(lead ~ ., family = binomial(link = \"logit\"), \n#'                         data = lead)\n#' est = summary( lead.logistic )$coef[2, c(1, 2)]\n#' \n#' OR       = exp(est[1])\n#' lowerOR  = exp(est[1] - 1.96*est[2])\n#' upperOR  = exp(est[1] + 1.96*est[2]) \n#' evalues.OR(OR, lowerOR, upperOR, rare=FALSE)\n\n\nevalues.OR = function( est, lo = NULL, hi = NULL, rare = NULL, true = 1 ) {\n    \n    # organize user's values\n    values = c(est, lo, hi)\n    \n    # sanity checks\n    if ( est < 0 ) stop(\"OR cannot be negative\")\n    if ( is.null(rare) ) stop(\"Must specify whether outcome is rare\")\n    \n    # if needed, convert to approximate RR\n    # if not rare, no conversion needed\n    if (rare){ \n        #warning(\"Results assume a rare outcome (<15% at end of follow-up)\")\n        truerR = true\n    }\n    else if ( ! rare ) {\n        #warning(\"Results assume a common outcome (>15% at end of follow-up)\")\n        values = sqrt(values)\n        \n        # convert true value to which to shift observed point estimate\n        #  to RR scale\n        truerR = sqrt(true)\n    }\n    \n    return( evalues.RR( values[1], values[2], values[3], true = truerR ) )\n}\n\n\n\n#' Compute E-value for a risk ratio or rate ratio and its confidence interval limits\n#' \n#' Returns a data frame containing point estimates, the lower confidence limit, and the upper confidence limit\n#' for the risk ratio (as provided by the user) as well as E-values for the point estimate and the confidence interval\n#' limit closer to the null.  \n#' @param est The point estimate\n#' @param lo The lower limit of the confidence interval\n#' @param hi The upper limit of the confidence interval\n#' @param true The true RR to which to shift the observed point estimate. Typically set to 1 to consider a null true effect. \n#' @export\n#' @examples\n#' # compute E-value for leukemia example in VanderWeele and Ding (2017)\n#' evalues.RR( 0.80, 0.71, 0.91 )\n#' \n#' # you can also pass just the point estimate\n#' evalues.RR( 0.80 )\n#' \n#' # demonstrate symmetry of E-value\n#' # this apparently causative association has same E-value as the above\n#' evalues.RR( 1 / 0.80 )\n\nevalues.RR = function( est, lo = NA, hi = NA, true = 1 ) {\n    \n    # organize user's values\n    values = c(est, lo, hi)\n    \n    # sanity checks\n    if ( est < 0 ) stop(\"RR cannot be negative\")\n    if ( true < 0 ) stop(\"True value is impossible\")\n    \n    # check if CI crosses null\n    null.CI = NA\n    if ( est > true & !is.na(lo) ) {\n        null.CI = ( lo < true )\n    }\n    \n    if ( est < true & !is.na(hi) ) {\n        null.CI = ( hi > true )\n    }\n    \n    \n    # sanity checks for CI\n    if ( !is.na(lo) & !is.na(hi) ) {\n        # check if lo < hi\n        if ( lo > hi ) stop(\"Lower confidence limit should be less than upper confidence limit\")\n    }\n    \n    if ( !is.na(lo) & est < lo ) stop(\"Point estimate should be inside confidence interval\")\n    if ( !is.na(hi) & est > hi ) stop(\"Point estimate should be inside confidence interval\")\n    \n    # compute E-values\n    E = sapply( values, FUN = function(x) threshold( x, true = true ) )\n    \n    \n    # clean up CI reporting\n    # if CI crosses null, set its E-value to 1\n    if ( !is.na(null.CI) & null.CI == TRUE ){\n        E[ 2:3 ] = 1\n        message(\"Confidence interval crosses the true value, so its E-value is 1.\") \n    }\n    \n    # if either CI limit is passed...\n    if ( !is.na(lo) | !is.na(hi) ) {\n        # ...then only report E-value for CI limit closer to null\n        if ( est > true ) E[3] = NA\n        if ( est < true ) E[2] = NA\n        if ( est == true ) {\n            E[2] = 1\n            E[3] = NA\n        }\n    }\n    \n    result = rbind(values, E)\n    \n    rownames(result) = c(\"RR\", \"E-values\")\n    colnames(result) = c(\"point\", \"lower\", \"upper\")\n    \n    result\n}\n\n\n#' Estimate risk ratio and compute CI limits from two-by-two table\n#'\n#' Given counts in a two-by-two table, computes risk ratio and confidence interval limits.  \n#' @param n11 Number exposed (X=1) and diseased (D=1)\n#' @param n10 Number exposed (X=1) and not diseased (D=0)\n#' @param n01 Number unexposed (X=0) and diseased (D=1)\n#' @param n00 Number unexposed (X=0) and not diseased (D=0)\n#' @param alpha Alpha level associated with confidence interval\n#' @export\n#' @import\n#' stats\n#' @examples\n#' # Hammond and Holl (1958 JAMA) Data\n#' # Two by Two Table\n#' #          Lung Cancer    No Lung Cancer\n#'# Smoker    397            78557\n#'# Nonsmoker 51             108778\n#'\n#'twoXtwoRR(397, 78557, 51, 108778)\n\ntwoXtwoRR = function(n11, n10, n01, n00, alpha = 0.05)\n{\n    p1     = n11/(n11 + n10)\n    p0     = n01/(n01 + n00)\n    RR     = p1/p0\n    logRR  = log(RR)\n    \n    selogRR  = sqrt( 1/n11 - 1/(n11+n10) + 1/n01 - 1/(n01+n00) )\n    q.alpha  = qnorm(1 - alpha/2)\n    \n    upperRR  = exp(logRR + q.alpha*selogRR)\n    lowerRR  = exp(logRR - q.alpha*selogRR)\n    \n    res         = c(RR, upperRR, lowerRR)\n    names(res)  = c(\"point\", \"upper\", \"lower\")\n    \n    return( res  ) \n}\n\n\n\n\n#' Compute E-value for single value of risk ratio\n#' \n#' Computes E-value for a single value of the risk ratio. Users should typically call the \n#' relevant \\code{evalues.XX()} function rather than this internal function.  \n#' @param x The risk ratio\n#' @param true The true RR to which to shift the observed point estimate. Typically set to 1 to consider a null true effect. \n#' @export\n#' @examples\n#' ## Example 1\n#' ## Hammond and Holl (1958 JAMA) Data\n#' ## Two by Two Table\n#' #          Lung Cancer    No Lung Cancer\n#'# Smoker    397            78557\n#'# Nonsmoker 51             108778\n#'\n#' # first get RR and CI bounds\n#' twoXtwoRR(397, 78557, 51, 108778)\n#' \n#' # then compute E-values\n#' evalues.RR(10.729780, 8.017457, 14.359688)\n\n\nthreshold = function(x, true = 1) {\n    \n    if ( is.na(x) ) return(NA)\n    \n    if(x < 0){\n        warning(\"The risk ratio must be non-negative.\")\n    }  \n    \n    if(x <= 1){\n        x = 1 / x\n        true = 1 / true\n    }\n    \n    if( true > x ) {\n        return(1)\n        #stop(\"The observed estimate is already smaller than the true value. No unmeasured confounding needed.\")\n    }\n    \n    return( ( x + sqrt( x * ( x - true ) ) ) / true )\n}\n\n\n\n\n\n\n\n#' Compute E-value for a population-standardized risk difference and its confidence interval limits\n#' \n#' Returns E-values for the point estimate and the lower confidence interval limit for a positive risk difference. \n#' If the risk difference is negative, the exposure coding should be first be reversed to yield a positive risk difference. \n#' @param n11 Number of exposed, diseased individuals\n#' @param n10 Number of exposed, non-diseased individuals\n#' @param n01 Number of unexposed, diseased individuals\n#' @param n00 Number of unexposed, non-diseased individuals\n#' @param true True value of risk difference to which to shift the point estimate. Usually set to 0 to consider the null. \n#' @param alpha Alpha level\n#' @param grid Spacing for grid search of E-value\n#' @export\n#' @import\n#' stats\n#' graphics\n#' @examples\n#' \n#' ## example 1     \n#' ## Hammond and Holl (1958 JAMA) Data\n#' ## Two by Two Table\n#' ##          Lung Cancer    No Lung Cancer\n#' ##Smoker    397            78557\n#' ##Nonsmoker 51             108778\n#' \n#' # E-value to shift observed risk difference to 0\n#' evalues.RD( 397, 78557, 51, 108778)\n#' \n#' # E-values to shift observed risk difference to other null values\n#' evalues.RD( 397, 78557, 51, 108778, true = 0.001)\n\n\nevalues.RD = function(n11, n10, n01, n00,  \n                      true = 0, alpha = 0.05, grid = 0.0001) {\n    \n    # sanity check\n    if ( any( c(n11, n10, n01, n00) < 0 ) ) stop(\"Negative cell counts are impossible.\")\n    \n    # sample sizes\n    N = n10 + n11 + n01 + n00\n    N1 = n10 + n11  # total X=1\n    N0 = n00 + n01  # total X=0\n    \n    # compute f = P( X = 1 )\n    f = N1 / N\n    \n    # P( D = 1 | X = 1 )\n    p1  = n11 / N1\n    \n    # P( D = 1 | X = 0 )\n    p0  = n01 / N0\n    \n    if(p1 < p0) stop(\"RD < 0; please relabel the exposure such that the risk difference > 0.\")\n    \n    \n    # standard errors\n    se.p1 = sqrt( p1 * (1-p1) / N1 )\n    se.p0 = sqrt( p0 * (1-p0) / N0 )\n    \n    # back to Peng's code\n    s2.f   = f*(1-f)/N\n    s2.p1  = se.p1^2\n    s2.p0  = se.p0^2\n    diff   = p0*(1-f) - p1*f\n    \n    # bias factor and E-value for point estimate\n    est.BF = ( sqrt( (true + diff)^2 + 4 * p1 * p0 * f * (1-f)  ) - (true + diff) ) / ( 2 * p0 * f )\n    est.Evalue    = threshold(est.BF)   \n    if(p1 - p0 <= true) est.Evalue = 1\n    \n    \n    # compute lower CI limit\n    Zalpha        = qnorm(1-alpha/2)  # critical value\n    lowerCI       = p1 - p0 - Zalpha*sqrt(s2.p1 + s2.p0)\n    \n    # check if CI contains null\n    if ( lowerCI <= true ) {\n        \n        # warning(\"Lower CI limit of RD is smaller than or equal to true value.\")\n        return(list(est.Evalue = est.Evalue, lower.Evalue = 1))\n        \n    } else {\n        # find E-value for lower CI limit\n        # we know it's less than or equal to E-value for point estimate\n        BF.search = seq(1, est.BF, grid)\n        \n        # population-standardized risk difference\n        RD.search = p1 - p0*BF.search\n        f.search  = f + (1-f)/BF.search\n        \n        Low.search = RD.search*f.search - Zalpha*sqrt(   \n            (s2.p1 + s2.p0*BF.search^2)*f.search^2 +\n                RD.search^2*(1 - 1/BF.search)^2*s2.f\n        )\n        \n        Low.ind    = (Low.search <= true)\n        \n        Low.no     = which(Low.ind==1)[1]\n        \n        lower.Evalue = threshold( BF.search[Low.no] )\n        \n        \n        return(list(est.Evalue   = est.Evalue,\n                    lower.Evalue = lower.Evalue))\n    }\n    \n}\n\n\n\n#' Plot bias factor as function of confounding relative risks\n#' \n#' Plots the bias factor required to explain away a provided relative risk. \n#' @param RR The relative risk \n#' @param xmax Upper limit of x-axis. \n#' @export\n#' @examples\n#' # recreate the plot in VanderWeele and Ding (2017)\n#' bias_plot(RR=3.9, xmax=20)\n\nbias_plot = function(RR, xmax) {\n    \n    x = seq(0, xmax, 0.01)\n    \n    # MM: reverse RR if it's preventive\n    if (RR < 1) RR = 1/RR\n    \n    plot(x, x, lty = 2, col = \"white\", type = \"l\", xaxs = \"i\", yaxs = \"i\", xaxt=\"n\", yaxt = \"n\",\n         xlab = expression(RR[EU]), ylab = expression(RR[UD]),\n         xlim = c(0,xmax),\n         main = \"\")\n    \n    x = seq(RR, xmax, 0.01)\n    \n    y    = RR*(RR-1)/(x-RR)+RR\n    \n    lines(x, y, type = \"l\")\n    \n    \n    high = RR + sqrt(RR*(RR-1))\n    \n    \n    points(high, high, pch = 19)\n    \n    label5 = seq(5, 40, by = 5)\n    axis(1, label5, label5, cex.axis = 1)\n    axis(2, label5, label5, cex.axis = 1)\n    \n    g = round( RR + sqrt( RR * (RR - 1) ), 2 )\n    label = paste(\"(\", g, \", \", g, \")\", sep=\"\")\n    \n    text(high + 3, high + 1, label)\n    \n    legend(\"bottomleft\", expression(\n        RR[EU]*RR[UD]/(RR[EU]+RR[UD]-1)==RR\n    ), \n    lty = 1:2,\n    bty = \"n\")\n    \n}\n\n\n\n\n############################ META-ANALYSIS FUNCTIONS ############################ \n\n\n#' Estimates and inference for sensitivity analyses\n#'\n#' Computes point estimates, standard errors, and confidence interval bounds\n#' for (1) \\code{prop}, the proportion of studies with true effect sizes above \\code{q} (or below\n#' \\code{q} for an apparently preventive \\code{yr}) as a function of the bias parameters;\n#' (2) the minimum bias factor on the relative risk scale (\\code{Tmin}) required to reduce to\n#' less than \\code{r} the proportion of studies with true effect sizes more extreme than\n#' \\code{q}; and (3) the counterpart to (2) in which bias is parameterized as the minimum\n#' relative risk for both confounding associations (\\code{Gmin}).\n#' @param q True effect size that is the threshold for \"scientific significance\"\n#' @param r For \\code{Tmin} and \\code{Gmin}, value to which the proportion of large effect sizes is to be reduced\n#' @param muB Mean bias factor on the log scale across studies\n#' @param sigB Standard deviation of log bias factor across studies\n#' @param yr Pooled point estimate (on log scale) from confounded meta-analysis\n#' @param vyr Estimated variance of pooled point estimate from confounded meta-analysis\n#' @param t2 Estimated heterogeneity (tau^2) from confounded meta-analysis\n#' @param vt2 Estimated variance of tau^2 from confounded meta-analysis\n#' @param CI.level Confidence level as a proportion\n#' @param tail \\code{above} for the proportion of effects above \\code{q}; \\code{below} for\n#' the proportion of effects below \\code{q}. By default, is set to \\code{above} for relative risks\n#' above 1 and to \\code{below} for relative risks below 1.\n#' @export\n#' @details\n#' To compute all three point estimates (\\code{prop, Tmin, and Gmin}) and inference, all\n#' arguments must be non-\\code{NULL}. To compute only a point estimate for \\code{prop},\n#' arguments \\code{r, vyr}, and \\code{vt2} can be left \\code{NULL}. To compute only\n#' point estimates for \\code{Tmin} and \\code{Gmin}, arguments \\code{muB, vyr}, and \\code{vt2}\n#' can be left \\code{NULL}. To compute inference for all point estimates, \\code{vyr} and \n#' \\code{vt2} must be supplied. \n#' @import metafor\n#' stats \n#' @examples\n#' d = metafor::escalc(measure=\"RR\", ai=tpos, bi=tneg,\n#' ci=cpos, di=cneg, data=metafor::dat.bcg)\n#' \n#' m = metafor::rma.uni(yi= d$yi, vi=d$vi, knha=FALSE,\n#'                      measure=\"RR\", method=\"DL\" ) \n#' yr = as.numeric(m$b)  # metafor returns on log scale\n#' vyr = as.numeric(m$vb)\n#' t2 = m$tau2\n#' vt2 = m$se.tau2^2 \n#' \n#' # obtaining all three estimators and inference\n#' confounded_meta( q=log(0.90), r=0.20, muB=log(1.5), sigB=0.1,\n#'                  yr=yr, vyr=vyr, t2=t2, vt2=vt2,\n#'                  CI.level=0.95 )\n#' \n#' # passing only arguments needed for prop point estimate\n#' confounded_meta( q=log(0.90), muB=log(1.5),\n#'                  yr=yr, t2=t2, CI.level=0.95 )\n#' \n#' # passing only arguments needed for Tmin, Gmin point estimates\n#' confounded_meta( q=log(0.90), r=0.20,\n#'                  yr=yr, t2=t2, CI.level=0.95 )\n\n\nconfounded_meta = function( q, r=NA, muB=NA, sigB=0,\n                            yr, vyr=NA, t2, vt2=NA,\n                            CI.level=0.95, tail=NA ) {\n    \n    # somewhere have option to plot the bias factor distribution, the confounded distribution, and the adjusted distribution\n    \n    ##### Check for Bad Input #####\n    if ( t2 < 0 ) stop(\"Heterogeneity cannot be negative\")\n    if ( sigB < 0 ) stop(\"Bias factor variance cannot be negative\")\n    \n    # the second condition is needed for Shiny app:\n    #  if user deletes the input in box, then it's NA instead of NULL\n    if ( ! is.na(vyr) ) {\n        if (vyr < 0) stop(\"Variance of point estimate cannot be negative\")\n    }\n    \n    if ( ! is.na(vt2) ) {\n        if (vt2 < 0) stop(\"Variance of heterogeneity cannot be negative\")\n    }\n    \n    if ( ! is.na(r) ) {\n        if (r < 0 | r > 1) stop(\"r must be between 0 and 1\")\n    }\n    \n    if ( t2 <= sigB^2 ) stop(\"Must have t2 > sigB^2\")\n    \n    \n    ##### Messages When Not All Output Can Be Computed #####\n    if ( is.na(vyr) | is.na(vt2) ) message(\"Cannot compute inference without vyr and vt2. Returning only point estimates.\")\n    if ( is.na(r) ) message(\"Cannot compute Tmin or Gmin without r. Returning only prop.\")\n    \n    ##### Point Estimates: Causative Case #####\n    \n    # if tail isn't provided, assume user wants the more extreme one (away from the null)\n    if ( is.na(tail) ) tail = ifelse( yr > log(1), \"above\", \"below\" )\n    \n    if ( tail == \"above\" ) {\n        \n        if ( !is.na(muB) ) {\n            # prop above\n            Z = ( q + muB - yr ) / sqrt( t2 - sigB^2 )\n            phat = 1 - pnorm(Z) \n        } else {\n            phat = NA\n        }\n        \n        if ( !is.null(r) ) {\n            \n            # min bias factor\n            # the max is there in case no bias is needed\n            # (i.e., proportion of effects > q already < r without confounding)\n            Tmin = max( 1, exp( qnorm(1-r) * sqrt(t2) - q + yr ) )\n            \n            # min confounding strength\n            Gmin = Tmin + sqrt( Tmin^2 - Tmin )\n        } else {\n            Tmin = Gmin = NA\n        }\n    }\n    \n    ##### Point Estimates: Preventive Case #####\n    else if ( tail == \"below\" ) {\n        \n        if ( !is.na(muB) ) {\n            # prop below\n            Z = ( q - muB - yr ) / sqrt( t2 - sigB^2 )\n            phat = pnorm(Z) \n        } else {\n            phat = NA\n        }\n        \n        if ( !is.na(r) ) {\n            # min bias factor\n            Tmin = exp( q - yr - qnorm(r) * sqrt(t2) )\n            \n            # min confounding strength\n            Gmin = Tmin + sqrt( Tmin^2 - Tmin )\n        } else {\n            Tmin = Gmin = NA\n        }\n    }\n    \n    \n    ##### Delta Method Inference: P-Hat #####\n    # do inference only if given needed SEs\n    if ( !is.na(vyr) & !is.na(vt2) & !is.na(muB) ){\n        \n        # term in numerator depends on whether causative or preventive RR\n        num.term = ifelse( yr > log(1), q + muB - yr, q - muB - yr )\n        \n        term1.1 = vyr / (t2 - sigB^2 )\n        term1.2 = ( vt2 * (num.term)^2 ) / ( 4 * (t2 - sigB^2 )^3 )\n        term1 = sqrt( term1.1 + term1.2 )\n        \n        Z = num.term / sqrt( t2 - sigB^2 )\n        SE = term1 * dnorm(Z)\n        \n        # confidence interval\n        tail.prob = ( 1 - CI.level ) / 2\n        lo.phat = max( 0, phat + qnorm( tail.prob )*SE )\n        hi.phat = min( 1, phat - qnorm( tail.prob )*SE )\n        \n    } else {\n        SE = lo.phat = hi.phat = NA\n    }\n    \n    ##### Delta Method Inference: Tmin and Gmin #####\n    # do inference only if given needed SEs and r\n    if ( !is.na(vyr) & !is.na(vt2) & !is.na(r) ){\n        \n        ##### Tmin #####\n        if (yr > log(1) ) {\n            term = ( vt2 * qnorm(1-r)^2 ) / ( 4 * t2 )\n            SE.T = exp( sqrt(t2) * qnorm(1-r) - q + yr ) * sqrt( vyr + term  )\n        } else {\n            term = ( vt2 * qnorm(r)^2 ) / ( 4 * t2 )\n            SE.T = exp( q - yr - sqrt(t2) * qnorm(r) ) * sqrt( vyr + term  )\n        }\n        \n        tail.prob = ( 1 - CI.level ) / 2\n        lo.T = max( 1, Tmin + qnorm( tail.prob )*SE.T )  # bias factor can't be < 1\n        hi.T = Tmin - qnorm( tail.prob )*SE.T  # but has no upper bound\n        \n        \n        ##### Gmin #####\n        SE.G = SE.T * ( 1 + ( 2*Tmin - 1 ) / ( 2 * sqrt( Tmin^2 - Tmin ) ) )\n        \n        lo.G = max( 1, Gmin + qnorm( tail.prob )*SE.G )  # confounding RR can't be < 1\n        hi.G = Gmin - qnorm( tail.prob )*SE.G  # but has no upper bound\n        \n    } else {  # i.e., user didn't pass parameters needed for inference\n        SE.T = SE.G = lo.T = lo.G = hi.T = hi.G = NA\n    }\n    \n    \n    # return results\n    res = data.frame( Value = c(\"Prop\", \"Tmin\", \"Gmin\"), \n                      Est = c( phat, Tmin, Gmin ),\n                      SE = c(SE, SE.T, SE.G),\n                      CI.lo = c(lo.phat, lo.T, lo.G), \n                      CI.hi = c(hi.phat, hi.T, hi.G) \n    )\n    \n    return(res)\n}\n\n\n\n\n\n\n\n#' Tables for sensitivity analyses\n#'\n#' Produces table showing the proportion of true effect sizes more extreme than \\code{q}\n#' across a grid of bias parameters \\code{muB} and \\code{sigB} (for \\code{meas == \"prop\"}).\n#' Alternatively, produces a table showing the minimum bias factor (for \\code{meas == \"Tmin\"})\n#' or confounding strength (for \\code{meas == \"Gmin\"}) required to reduce to less than\n#' \\code{r} the proportion of true effects more extreme than \\code{q}.\n#' @param meas \\code{prop}, \\code{Tmin}, or \\code{Gmin}\n#' @param q True effect size that is the threshold for \"scientific significance\"\n#' @param r For \\code{Tmin} and \\code{Gmin}, vector of values to which the proportion of large effect sizes is to be reduced\n#' @param muB Mean bias factor on the log scale across studies\n#' @param sigB Standard deviation of log bias factor across studies\n#' @param yr Pooled point estimate (on log scale) from confounded meta-analysis\n#' @param t2 Estimated heterogeneity (tau^2) from confounded meta-analysis\n#' @keywords meta-analysis, confounding, sensitivity\n#' @export\n#' @details\n#' For \\code{meas==\"Tmin\"} or \\code{meas==\"Gmin\"}, arguments \\code{muB} and\n#' \\code{sigB} can be left \\code{NULL}; \\code{r} can also be \\code{NULL} as\n#' it will default to a reasonable range of proportions. Returns a \\code{data.frame}\n#' whose rows are values of \\code{muB} (for \\code{meas==\"prop\"}) or of \\code{r} \n#' (for \\code{meas==\"Tmin\"} or \\code{meas==\"Gmin\"}). Its columns are values of \n#' \\code{sigB} (for \\code{meas==\"prop\"}) or of \\code{q} (for \\code{meas==\"Tmin\"}\n#' or \\code{meas==\"Gmin\"}).\n#' Tables for \\code{Gmin} will display \\code{NaN} for cells corresponding to \\code{Tmin}<1,\n#' i.e., for which no bias is required to reduce the effects as specified. \n#' \n#' @import ggplot2 \n#' @examples\n#' sens_table( meas=\"prop\", q=log(1.1), muB=c( log(1.1),\n#' log(1.5), log(2.0) ), sigB=c(0, 0.1, 0.2), \n#' yr=log(2.5), t2=0.1 )\n#' \n#' sens_table( meas=\"Tmin\", q=c( log(1.1), log(1.5) ),\n#' yr=log(1.3), t2=0.1 ) \n#' \n#' # Tmin is 1 here because we already have <80% of effects\n#' #  below log(1.1) even without any confounding\n#' sens_table( meas=\"Gmin\", r=0.8, q=c( log(1.1) ),\n#' yr=log(1.3), t2=0.1 )\n\n\nsens_table = function( meas, q, r=seq(0.1, 0.9, 0.1), muB=NULL, sigB=NULL,\n                       yr, t2 ) {\n    \n    ##### Check for Correct Inputs Given Measure ######\n    if ( meas==\"prop\" & ( is.null(muB) | is.null(sigB) ) ) {\n        stop( \"To compute proportion above q, provide muB and sigB\")\n    }\n    \n    if ( meas==\"prop\" & length(q) > 1 ) {\n        stop( \"To compute proportion above q, provide only a single value of q\" )\n    }\n    \n    ###### Generate Table #####\n    \n    # table skeleton\n    nrow = ifelse( meas==\"prop\", length(muB), length(r) )\n    ncol = ifelse( meas==\"prop\", length(sigB), length(q) )\n    \n    m = matrix( NA, nrow=nrow, ncol=ncol )\n    \n    # fill in each cell individually\n    # doing this inefficient thing because confounded_meta is not vectorized\n    # because it returns a dataframe\n    for (i in 1:nrow) {\n        for (j in 1:ncol) {\n            if ( meas == \"prop\" ) {\n                m[i,j] = suppressMessages( confounded_meta( q=q, muB = muB[i], sigB = sigB[j],\n                                                            yr=yr, t2=t2 )[1,\"Est\"] )\n            } else if ( meas == \"Tmin\" ) {\n                m[i,j] = suppressMessages( confounded_meta( q=q[j], r=r[i],\n                                                            yr=yr, t2=t2 )[2,\"Est\"] )\n            } else if ( meas == \"Gmin\" ) {\n                m[i,j] = suppressMessages( confounded_meta( q=q[j], r=r[i],\n                                                            yr=yr, t2=t2 )[3,\"Est\"] )\n            }\n            \n        }\n    }\n    \n    d = data.frame(m)\n    \n    if ( meas==\"prop\" ) {\n        row.names(d) = paste( \"muB=\", round( muB, 3 ), sep=\"\" )\n    } else if ( meas %in% c( \"Tmin\", \"Gmin\" ) ) {\n        row.names(d) = round( r, 3 )\n    }\n    \n    if( meas==\"prop\" ) {\n        names(d) = paste( \"sigB=\", round( sigB, 3 ), sep=\"\" )  \n    } else if ( meas %in% c( \"Tmin\", \"Gmin\" ) ) {\n        names(d) = round( q, 3 )\n    }\n    \n    return(d)\n}\n\n\n\n\n#' Plots for sensitivity analyses\n#'\n#' Produces line plots (\\code{type=\"line\"}) showing the bias factor on the relative risk (RR) scale vs. the proportion\n#' of studies with true RRs above \\code{q} (or below it for an apparently preventive relative risk).\n#' The plot secondarily includes a X-axis scaled based on the minimum strength of confounding\n#' to produce the given bias factor. The shaded region represents a 95\\% pointwise confidence band.\n#' Alternatively, produces distribution plots (\\code{type=\"dist\"}) for a specific bias factor showing the observed and \n#' true distributions of RRs with a red line marking exp(\\code{q}).\n#' @param type \\code{dist} for distribution plot; \\code{line} for line plot (see Details)\n#' @param q True effect size that is the threshold for \"scientific significance\"\n#' @param muB Single mean bias factor on log scale (only needed for distribution plot)\n#' @param Bmin Lower limit of lower X-axis on the log scale (only needed for line plot)\n#' @param Bmax Upper limit of lower X-axis on the log scale (only needed for line plot)\n#' @param sigB Standard deviation of log bias factor across studies (length 1)\n#' @param yr Pooled point estimate (on log scale) from confounded meta-analysis\n#' @param vyr Estimated variance of pooled point estimate from confounded meta-analysis\n#' @param t2 Estimated heterogeneity (tau^2) from confounded meta-analysis\n#' @param vt2 Estimated variance of tau^2 from confounded meta-analysis\n#' @param breaks.x1 Breaks for lower X-axis (bias factor) on RR scale (optional for line plot; not used for distribution plot)\n#' @param breaks.x2 Breaks for upper X-axis (confounding strength) on RR scale (optional for line plot; not used for distribution plot)\n#' @param CI.level Pointwise confidence level as a proportion\n#' @keywords meta-analysis, confounding, sensitivity\n#' @details\n#' Arguments \\code{vyr} and \\code{vt2} can be left \\code{NULL}, in which case no confidence\n#' band will appear on the line plot. \n#' @export\n#' @import ggplot2 \n#' @examples\n#' # with variable bias and with confidence band\n#' sens_plot( type=\"line\", q=log(1.1), Bmin=log(1), Bmax=log(4), sigB=0.1,\n#'            yr=log(1.3), vyr=0.005, t2=0.4, vt2=0.03 )\n#' \n#' # with fixed bias and without confidence band\n#' sens_plot( type=\"line\", q=log(1.1), Bmin=log(1), Bmax=log(4),\n#'            yr=log(1.3), t2=0.4 )\n#' \n#' # apparently preventive\n#' sens_plot( type=\"line\", q=log(0.90), Bmin=log(1), Bmax=log(4),\n#'            yr=log(0.6), vyr=0.005, t2=0.4, vt2=0.04 )\n#' \n#' # distribution plot: apparently causative\n#' # commented out because takes 5-10 seconds to run\n#' # sens_plot( type=\"dist\", q=log(1.1), muB=log(2),\n#' #           yr=log(1.3), t2=0.4 )\n#'            \n#' # distribution plot: apparently preventive\n#' # commented out because takes 5-10 seconds to run\n#' # sens_plot( type=\"dist\", q=log(0.90), muB=log(1.5),\n#' #           yr=log(0.7), t2=0.2 )\n\n\nsens_plot = function( type, q, muB=NULL, Bmin=log(1), Bmax=log(5), sigB=0,\n                      yr, vyr=NULL, t2, vt2=NULL,\n                      breaks.x1=NULL, breaks.x2=NULL,\n                      CI.level=0.95 ) {\n    \n    ##### Check for Bad Input ######\n    if ( type==\"dist\" ) {\n        \n        if( is.null(muB) ) stop(\"For type='dist', must provide muB\")\n        \n        if ( ( length(muB) > 1 ) | ( length(sigB) > 1 ) ) {\n            stop( \"For type='dist', muB and sigB must be length 1\")\n        }\n    }\n    \n    if ( type==\"line\" ) {\n        \n        if ( is.null(vyr) | is.null(vt2) ) {\n            message( \"No confidence interval because vyr or vt2 is NULL\")\n        }\n    }\n    \n    ##### Distribution Plot ######\n    if ( type==\"dist\" ) {\n        \n        # simulate confounded distribution\n        reps = 10000\n        RR.c = exp( rnorm( n=reps, mean=yr, sd=sqrt(t2) ) )\n        \n        # simulate unconfounded distribution\n        Mt = ifelse( yr > 0, yr - muB, yr + muB )\n        RR.t = exp( rnorm( n=reps, mean=Mt, sd=sqrt(t2-sigB^2) ) )\n        \n        # get reasonable limits for X-axis\n        x.min = min( quantile(RR.c, 0.01), quantile(RR.t, 0.01) )\n        x.max = max( quantile(RR.c, 0.99), quantile(RR.t, 0.99) )\n        \n        temp = data.frame( group = rep( c( \"Observed\", \"True\" ), each = reps ), \n                           val = c( RR.c, RR.t ) )\n        \n        colors=c(\"black\", \"orange\")\n        p = ggplot2::ggplot( data=temp, aes(x=temp$val, group=temp$group ) ) +\n            geom_density( aes( fill=temp$group ), alpha=0.4 ) +\n            theme_bw() + xlab(\"Study-specific relative risks\") +\n            ylab(\"\") + guides(fill=guide_legend(title=\" \")) +\n            scale_fill_manual(values=colors) +\n            geom_vline( xintercept = exp(q), lty=2, color=\"red\" ) +\n            scale_x_continuous( limits=c(x.min, x.max), breaks = seq( round(x.min), round(x.max), 0.5) ) +\n            ggtitle(\"Observed and true relative risk distributions\")\n        \n        graphics::plot(p)\n    }\n    \n    ##### Line Plot ######\n    if ( type==\"line\" ) {\n        # get mean bias factor values for a bunch of different B's\n        t = data.frame( B = seq(Bmin, Bmax, .01), phat = NA, lo = NA, hi = NA )\n        t$eB = exp(t$B)\n        \n        for ( i in 1:dim(t)[1] ) {\n            # r is irrelevant here\n            cm = confounded_meta(q, r=0.10, muB=t$B[i], sigB,\n                                 yr, vyr, t2, vt2,\n                                 CI.level=CI.level)\n            t$phat[i] = cm$Est[ cm$Value==\"Prop\" ]\n            t$lo[i] = cm$CI.lo[ cm$Value==\"Prop\" ]\n            t$hi[i] = cm$CI.hi[ cm$Value==\"Prop\" ]\n        }\n        \n        # compute values of g for the dual X-axis\n        if ( is.null(breaks.x1) ) breaks.x1 = seq( exp(Bmin), exp(Bmax), .5 )\n        if ( is.null(breaks.x2) ) breaks.x2 = round( breaks.x1 + sqrt( breaks.x1^2 - breaks.x1 ), 2)\n        \n        # define transformation in a way that is monotonic over the effective range of B (>1)\n        # to avoid ggplot errors\n        g = Vectorize( function(x) {\n            if (x < 1) return( x / 1e10 )\n            x + sqrt( x^2 - x )\n        } )\n        \n        p = ggplot2::ggplot( t, aes(x=t$eB, y=t$phat ) ) + theme_bw() +\n            scale_y_continuous( limits=c(0,1), breaks=seq(0, 1, .1)) +\n            scale_x_continuous(  breaks = breaks.x1,\n                                 sec.axis = sec_axis( ~ g(.),  # confounding strength axis\n                                                      name = \"Minimum strength of both confounding RRs\",\n                                                      breaks=breaks.x2 ) ) +\n            geom_line(lwd=1.2) +\n            xlab(\"Bias factor (RR scale)\") +\n            ylab( paste( ifelse( yr > log(1),\n                                 paste( \"Estimated proportion of studies with true RR >\", round( exp(q), 3 ) ),\n                                 paste( \"Estimated proportion of studies with true RR <\", round( exp(q), 3 ) ) ) ) )\n        \n        # can't compute a CI if the bounds aren't there\n        no.CI = any( is.na(t$lo) ) | any( is.na(t$hi) )\n        \n        if ( no.CI ) graphics::plot(p)\n        else p + ggplot2::geom_ribbon( aes(ymin=t$lo, ymax=t$hi), alpha=0.15 )   \n        \n    }\n}\n\n\n\n#' Convert forest plot or summary table to meta-analytic dataset\n#'\n#' Given relative risks (RR) and upper bounds of 95\\% confidence intervals (CI)\n#' from a forest plot or summary table, returns a dataframe ready for meta-analysis\n#' (e.g., via the \\code{metafor} package) with the log-RRs and their variances.\n#' Optionally, the user may indicate studies for which the point estimate is to be\n#' interpreted as an odds ratios of a common outcome rather than a relative risk;\n#' for such studies, the function applies VanderWeele (2017)'s square-root transformation to convert\n#' the odds ratio to an approximate risk ratio. \n#' @param type \\code{RR} if point estimates are RRs or ORs (to be handled on log scale); \\code{raw} if point estimates are raw differences, standardized mean differences, etc. (such that they can be handled with no transformations)\n#' @param est Vector of study point estimates on RR or OR scale\n#' @param hi Vector of upper bounds of 95\\% CIs on RRs\n#' @param sqrt Vector of booleans (TRUE/FALSE) for whether each study measured an odds ratio of a common outcome that should be approximated as a risk ratio via the square-root transformation\n#' @export\n#' @import stats\n\nscrape_meta = function( type=\"RR\", est, hi, sqrt=FALSE ){\n    \n    if ( type == \"RR\" ) {\n        # take square root for certain elements\n        RR = est\n        RR[sqrt] = sqrt( RR[sqrt] )\n        \n        # same for upper CI limit\n        hi.RR = hi\n        hi.RR[sqrt] = sqrt( hi.RR[sqrt] )\n        \n        sei = ( log(hi.RR) - log(RR) ) / qnorm(.975)\n        \n        return( data.frame( yi = log(RR), vyi = sei^2 ) )\n        \n    } else if ( type == \"raw\" ) {\n        \n        sei = ( hi - est ) / qnorm(.975)\n        return( data.frame( yi = est, vyi = sei^2 ) )\n    }\n    \n    \n}\n\n\n\n#' Estimate proportion of population effect sizes above or below a threshold\n#'\n#' Estimates the proportion of true effect sizes in a meta-analysis above or below\n#' a specified threshold of scientific importance. Effect sizes may be of any type (they need not\n#' be relative risks). This is a wrapper for \\code{confounded_meta}; it is the special case in\n#' which there is no unmeasured confounding. \n#' @param q True effect size that is the threshold for \"scientific importance\"\n#' @param yr Pooled point estimate from meta-analysis\n#' @param vyr Estimated variance of pooled point estimate from meta-analysis\n#' @param t2 Estimated heterogeneity (tau^2) from meta-analysis\n#' @param vt2 Estimated variance of tau^2 from meta-analysis\n#' @param CI.level Confidence level as a proportion\n#' @param tail \\code{above} for the proportion of effects above \\code{q}; \\code{below} for\n#' the proportion of effects below \\code{q}.\n#' @export\n\nstronger_than = function( q, yr, vyr=NULL, t2, vt2=NULL,\n                          CI.level=0.95, tail ) {\n    \n    # suppress warnings about lack of info for doing sensitivity analysis\n    # since we are not dealing with confounding \n    cm = suppressMessages( confounded_meta( q = q, muB = 0, sigB = 0,\n                                            yr = yr, vyr = vyr,\n                                            t2 = t2, vt2 = vt2,\n                                            CI.level = CI.level,\n                                            tail = tail ) )\n    \n    # return just the first row (proportion) since the rest are for sensitivity analyses\n    return( cm[1,] ) \n}\n\n\n\n\n\n",
    "created" : 1509471784428.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1407182820",
    "id" : "B5EAD9F5",
    "lastKnownWriteTime" : 1509572180,
    "path" : "~/Dropbox/Personal computer/HARVARD/THESIS/Thesis paper #1 (Metasens)/R code/Shiny app/GUI/meta_gui_2/startup.R",
    "project_path" : "startup.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}